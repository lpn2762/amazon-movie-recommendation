{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1JH5J1KQAUBMP</td>\n",
       "      <td>B00005U0JX</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1RSXP7MB772E3</td>\n",
       "      <td>B001DHXT1G</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AJGP5XYKKBGBG</td>\n",
       "      <td>0792840054</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2SQJPUCZNHMZE</td>\n",
       "      <td>B005LAIHSG</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3QVAKVRAH657N</td>\n",
       "      <td>B00005K3OU</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user        item  rating\n",
       "0  A1JH5J1KQAUBMP  B00005U0JX    True\n",
       "1  A1RSXP7MB772E3  B001DHXT1G    True\n",
       "2   AJGP5XYKKBGBG  0792840054    True\n",
       "3  A2SQJPUCZNHMZE  B005LAIHSG    True\n",
       "4  A3QVAKVRAH657N  B00005K3OU   False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('../data/reviews_sample_100.csv').drop(['Unnamed: 0', 'reviewTime'], axis = 1)\n",
    "ratings.columns = ['item', 'user', 'rating']\n",
    "ratings = ratings[['user', 'item', 'rating']]\n",
    "ratings['rating'] = ratings['rating'].astype(int)\n",
    "ratings['rating'] = (ratings['rating'] >= 4).astype(bool)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PLSI():\n",
    "    \n",
    "    def __init__(self, n_factors = 5, n_iters = 5, threshold = 0.5, optimize_threshold = False,\n",
    "                 verbose = False, user = 'user', item = 'item', rating = 'rating'):\n",
    "        self.n_items = 0\n",
    "        self.n_users = 0\n",
    "        self.n_latent_factors = n_factors\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.threshold = threshold\n",
    "        self.b_optimize_threshold = optimize_threshold\n",
    "        \n",
    "        self.user = user\n",
    "        self.item = item\n",
    "        self.rating = rating\n",
    "        \n",
    "    def _make_bool_matrix(self):\n",
    "        self.data.loc[:,self.rating] = self.data[self.rating].astype(bool)\n",
    "        data_matrix = self.data.pivot_table(index=self.user, columns=self.item).fillna(False)\n",
    "        return data_matrix\n",
    "        \n",
    "    def _train_initialize(self, data):\n",
    "        \n",
    "        self.data = data[data[self.rating] > 0]\n",
    "        self.data_zero = data.copy()\n",
    "        self.data_matrix = self._make_bool_matrix()\n",
    "        print('num users:', self.data_matrix.shape[0])\n",
    "        print('num items:', self.data_matrix.shape[1])\n",
    "        \n",
    "        print('proportion positive:', len(self.data) / len(self.data_zero))\n",
    "\n",
    "        self.user_array = np.array(self.data_matrix.index)\n",
    "        self.item_array = np.array(self.data_matrix.columns.levels[-1])\n",
    "        \n",
    "        self.n_users = len(self.user_array)\n",
    "        self.n_items = len(self.item_array)\n",
    "        self.n_impl_ratings = self.data[self.rating].sum()\n",
    "        \n",
    "        self.items_dict = defaultdict(list)\n",
    "        for index, value in enumerate(self.item_array):\n",
    "            self.items_dict[value] = index\n",
    "            \n",
    "        self.users_dict = defaultdict(list)\n",
    "        for index, value in enumerate(self.user_array):\n",
    "            self.users_dict[value] = index\n",
    "            \n",
    "        print('')\n",
    "            \n",
    "    def _param_initialize(self):\n",
    "        self.prob_z_given_user = np.random.rand(self.n_users, self.n_latent_factors)\n",
    "        self.prob_item_given_z = np.random.rand(self.n_latent_factors, self.n_items)\n",
    "        self.prob_z_given_user_item = np.random.rand(self.n_latent_factors)\n",
    "        \n",
    "        user_group = self.data.groupby(self.user)\n",
    "        self.count_user = user_group[self.rating].sum()\n",
    "        self.prob_user = self.count_user / self.n_impl_ratings\n",
    "        \n",
    "        item_group = self.data.groupby(self.item)\n",
    "        self.count_item = item_group[self.rating].sum()\n",
    "    \n",
    "    def _update_params(self):\n",
    "        \n",
    "        self.prob_item_given_user = np.dot(self.prob_z_given_user, self.prob_item_given_z)\n",
    "        \n",
    "        for z in range(self.n_latent_factors):\n",
    "            print('\\n===========Z={}==========='.format(z))\n",
    "            \n",
    "            print('\\n=======E-step=======')\n",
    "            \n",
    "            prob_z_given_user_item = np.dot(self.prob_z_given_user[:,z].reshape(1, -1).T,\\\n",
    "                                            self.prob_item_given_z[z,:].reshape(1, -1)) / \\\n",
    "                                            self.prob_item_given_user\n",
    "            \n",
    "            data_matrix_z = self.data_matrix * prob_z_given_user_item\n",
    "            \n",
    "            print('\\n=======LOOP 1=======')\n",
    "            prob_z_given_user_num_array = data_matrix_z.sum(axis = 1)\n",
    "            prob_z_given_user_den_array = self.data_matrix.sum(axis = 1)\n",
    "            \n",
    "            prob_z_given_user_array = prob_z_given_user_num_array.values / prob_z_given_user_den_array\n",
    "            self.prob_z_given_user[:,z] = prob_z_given_user_array\n",
    "            \n",
    "            print('\\n=======LOOP 2=======')\n",
    "            prob_item_given_z_den = data_matrix_z.sum().sum()\n",
    "            prob_item_given_z_num_array = data_matrix_z.sum(axis = 0)\n",
    "            prob_item_given_z_array = prob_item_given_z_num_array / prob_item_given_z_den\n",
    "            self.prob_item_given_z[z, :] = prob_item_given_z_array\n",
    "            clear_output()\n",
    "\n",
    "    def _calc_log_likelihood(self):\n",
    "\n",
    "        summand_1 = (self.data_matrix * np.log(self.prob_item_given_user)).sum().sum()\n",
    "        prob_user_array = (self.data_matrix.sum(axis = 1) / self.data_matrix.sum().sum()).values\n",
    "        count_user_array = self.data_matrix.sum(axis = 1)\n",
    "        \n",
    "        summand_2 = (count_user_array * np.log(prob_user_array)).sum()\n",
    "        log_likelihood = summand_1 + summand_2\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def _calc_reconstruction(self):\n",
    "        \n",
    "        total_instances = self.data_matrix.sum().sum()\n",
    "        \n",
    "        prob_user_array = (self.data_matrix.sum(axis = 1) / self.data_matrix.sum().sum()).values.reshape(1, -1).T\n",
    "        self.prob_joint_user_item = self.prob_item_given_user * prob_user_array * total_instances\n",
    "        return self.prob_joint_user_item\n",
    "    \n",
    "    def _train_predict(self):\n",
    "        \n",
    "        self._calc_reconstruction()\n",
    "        pred_list = list()\n",
    "        for row_index in range(len(self.data_zero)):\n",
    "            row = self.data_zero.iloc[row_index, :]\n",
    "            user = row[self.user]\n",
    "            item = row[self.item]\n",
    "            rating = row[self.rating]\n",
    "            \n",
    "            item_index = self.items_dict[item]\n",
    "            user_index = self.users_dict[user]\n",
    "            \n",
    "            if isinstance(item_index, list) or isinstance(user_index, list):\n",
    "                rating_pred = 0\n",
    "            else:\n",
    "                rating_pred = self.prob_joint_user_item[user_index][item_index]\n",
    "            pred_list.append(rating_pred)\n",
    "            \n",
    "        pred_array = np.array(pred_list)\n",
    "        ratings_array = np.array(self.data_zero[self.rating])\n",
    "        \n",
    "        return pred_array, ratings_array\n",
    "    \n",
    "    def _display_metrics(self):\n",
    "        pred_array, ratings_array = self._train_predict()\n",
    "        \n",
    "        mae = np.abs(self.data_matrix - self.prob_joint_user_item).sum().sum() / \\\n",
    "            self.data_matrix.size\n",
    "        precision = sum((pred_array >= self.threshold) & (ratings_array == 1)) / sum(pred_array >= self.threshold)\n",
    "        recall = sum((pred_array >= self.threshold) & (ratings_array == 1)) / sum(ratings_array)\n",
    "        accuracy = sum((pred_array >= self.threshold) == (ratings_array == 1)) / len(ratings_array)\n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        print('mae:', mae)\n",
    "        print('accuracy:', accuracy)\n",
    "        print('f1-score:', f1_score)\n",
    "        print('precision:', precision)\n",
    "        print('recall:', recall)\n",
    "    \n",
    "    def optimize_threshold(self, data, upper = 0.1, steps = 100, target_precision = 0.9):\n",
    "        print('\\n=========optimizing threshold=========')\n",
    "        ratings_array = data[self.rating]\n",
    "        pred_array = self.predict_proba(data)\n",
    "        threshold_array = np.linspace(0, upper, steps)[1:]\n",
    "        \n",
    "        best_threshold = 0\n",
    "        for threshold in threshold_array:\n",
    "            precision = sum((pred_array >= threshold) & (ratings_array == 1)) / sum(pred_array >= threshold)\n",
    "            recall = sum((pred_array >= threshold) & (ratings_array == 1)) / sum(ratings_array)\n",
    "            f1_score = (2 * precision * recall) / (precision + recall)\n",
    "            if precision >= target_precision:\n",
    "                best_threshold = threshold\n",
    "                break\n",
    "                \n",
    "        self.threshold = best_threshold\n",
    "        print('optimal threshold:', best_threshold)\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self._train_initialize(data)\n",
    "        self._param_initialize()\n",
    "        \n",
    "        for i in range(self.n_iters):\n",
    "            self._update_params()\n",
    "        \n",
    "            if self.verbose:\n",
    "                print('\\n==================ITER {}=================='.format(i+1))\n",
    "                self._display_metrics()\n",
    "                \n",
    "                log_l = self._calc_log_likelihood()\n",
    "                print('log-likelihood:', log_l)\n",
    "                \n",
    "        if self.b_optimize_threshold:\n",
    "            self._optimize_threshold()\n",
    "        if not self.verbose:\n",
    "            self._display_metrics()\n",
    "    \n",
    "    def predict_proba(self, data):\n",
    "        pred_list = list()\n",
    "        for row_index in range(len(data)):\n",
    "            row = data.iloc[row_index, :]\n",
    "            user = row[self.user]\n",
    "            item = row[self.item]\n",
    "            #rating = row[self.rating]\n",
    "            \n",
    "            item_index = self.items_dict[item]\n",
    "            user_index = self.users_dict[user]\n",
    "            \n",
    "            if isinstance(item_index, list) or isinstance(user_index, list):\n",
    "                rating_pred = 0\n",
    "            else:\n",
    "                rating_pred = self.prob_joint_user_item[user_index][item_index]\n",
    "            pred_list.append(rating_pred)\n",
    "            \n",
    "        pred_array = np.array(pred_list)\n",
    "        #ratings_array = np.array(self.data_zero[self.rating])\n",
    "        \n",
    "        return pred_array#, ratings_array\n",
    "        \n",
    "    def predict(self, data):\n",
    "        pred_array = self.predict_proba(data)\n",
    "        return pred_array >= self.threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 0.0010037195998\n",
      "accuracy: 0.9776\n",
      "f1-score: 0.984577251446\n",
      "precision: 0.971203477316\n",
      "recall: 0.998324490366\n"
     ]
    }
   ],
   "source": [
    "ratings_train = ratings[:10000]\n",
    "\n",
    "plsi = PLSI(n_factors = 10, n_iters = 10, threshold = 0.001, verbose = False)\n",
    "plsi.fit(ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========optimizing threshold=========\n",
      "optimal threshold: 1.0101010101e-06\n"
     ]
    }
   ],
   "source": [
    "plsi.optimize_threshold(ratings_test, steps=100, upper=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results == ratings_test['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = plsi._calc_reconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35807"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_test['rating'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.287666666667\n",
      "precision\n",
      "0.869565217391\n",
      "recall\n",
      "0.00928505106778\n"
     ]
    }
   ],
   "source": [
    "plsi.threshold = 0.02\n",
    "ratings_test = ratings[10000:13000]\n",
    "results = plsi.predict(ratings_test)\n",
    "print('accuracy')\n",
    "print(sum(results == ratings_test['rating']) / len(ratings_test))\n",
    "print('precision')\n",
    "print(sum(results & (ratings_test['rating'] == True)) / sum(results))\n",
    "print('recall')\n",
    "print(sum(results & (ratings_test['rating'] == True)) / sum(ratings_test['rating'] == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num users: 6\n",
      "num items: 5\n",
      "proportion positive: 0.35714285714285715\n",
      "\n",
      "mae: 0.0358382091568\n",
      "accuracy: 1.0\n",
      "f1-score: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.94,  0.94,  0.92,  0.1 ,  0.1 ],\n",
       "       [ 1.  ,  0.99,  0.97,  0.02,  0.02],\n",
       "       [ 1.  ,  0.99,  0.97,  0.02,  0.02],\n",
       "       [ 0.02,  0.03,  0.04,  0.96,  0.96],\n",
       "       [ 0.02,  0.03,  0.04,  0.96,  0.96],\n",
       "       [ 0.02,  0.03,  0.04,  0.96,  0.96]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "df_6 = pd.DataFrame([[1,1,1,0,0,0],\n",
    "                [1,1,1,0,0,0],\n",
    "                [1,1,1,0,0,0],\n",
    "                [0,0,0,1,1,0],\n",
    "                [0,0,0,1,1,0],\n",
    "                [0,0,0,1,1,0],\n",
    "                [0,0,0,0,0,0]],\n",
    "                columns = list('abcdef'), index = list('qrstuvw')).reset_index()\n",
    "df_6 = df_6.melt(value_vars=list('abcdef'), id_vars = 'index')\n",
    "df_6.columns = ['user', 'item', 'rating']\n",
    "df_6['rating'] = df_6['rating'].astype(bool)\n",
    "#df_6 = df_6.sample(38, replace=False)\n",
    "\n",
    "plsi = PLSI(n_factors = 2, n_iters = 8)\n",
    "plsi.fit(df_6)\n",
    "\n",
    "np.round(plsi._calc_reconstruction(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "5     True\n",
       "6     True\n",
       "7     True\n",
       "8     True\n",
       "9     True\n",
       "10    True\n",
       "11    True\n",
       "12    True\n",
       "13    True\n",
       "14    True\n",
       "15    True\n",
       "16    True\n",
       "17    True\n",
       "18    True\n",
       "19    True\n",
       "20    True\n",
       "21    True\n",
       "22    True\n",
       "23    True\n",
       "24    True\n",
       "25    True\n",
       "26    True\n",
       "27    True\n",
       "28    True\n",
       "29    True\n",
       "30    True\n",
       "31    True\n",
       "32    True\n",
       "33    True\n",
       "34    True\n",
       "35    True\n",
       "36    True\n",
       "37    True\n",
       "38    True\n",
       "39    True\n",
       "40    True\n",
       "41    True\n",
       "Name: rating, dtype: bool"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plsi.predict(df_6) == df_6['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(plsi.calc_reconstruction() >= 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316.606014110446"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings) / 10000 * (end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "total_prob = 0\n",
    "for i in range(plsi.n_latent_factors):\n",
    "    total_prob += plsi.prob_z_given_user_item[i]['A1GHUN5HXMHZ89']['076400459X']\n",
    "print(total_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
